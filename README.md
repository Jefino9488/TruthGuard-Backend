# TruthGuard Backend

The TruthGuard Backend is a Flask-based application designed to scrape news articles, analyze them using Google's Gemini AI for bias, credibility, and misinformation, and provide a comprehensive API for accessing and searching this data. It integrates with MongoDB for data storage, including support for vector search capabilities.

## Features

* **News Scraping**: Fetches top headlines and articles by topic from NewsAPI.
* **AI-Powered Analysis**: Utilizes Google's Gemini AI to perform detailed analysis of articles, including:
    * Bias analysis (overall score, political leaning, language, source, framing, selection, confirmation bias).
    * Misinformation analysis (risk score, fact checks, red flags, logical fallacies, evidence quality).
    * Sentiment analysis (overall sentiment, emotional tone, key phrases).
    * Credibility assessment (overall score, evidence quality, source reliability, logical consistency).
    * Narrative analysis (primary/secondary frames, narrative patterns, actor portrayal).
    * Technical analysis (readability, complexity, word count, key topics, named entities).
    * Recommendations for verification, alternative sources, and bias mitigation.
* **Database Integration**: Stores processed articles and their analysis results in MongoDB.
* **RESTful API**: Exposes various endpoints for managing, searching, and retrieving article data, including specific endpoints for high-bias or high-misinformation-risk articles.
* **Scheduled Tasks**: Automatically triggers news scraping and AI analysis at defined intervals using APScheduler.
* **Vector Search**: Supports semantic search of articles using MongoDB Atlas Vector Search based on embeddings generated by Sentence-Transformers.
* **Health Check**: Provides an endpoint to monitor the status of database connections and API keys.

## Technologies Used

* **Backend Framework**: Flask
* **Database**: MongoDB (via PyMongo)
* **AI/ML**:
    * Google Gemini API (via `google-generativeai`)
    * Sentence-Transformers for embeddings
* **News Scraping**:
    * NewsAPI (via `newsapi-python`)
    * Newspaper3k for full content extraction
* **Task Scheduling**: APScheduler
* **Dependency Management**: `pip` (using `requirements.txt`)
* **Environment Variables**: `python-dotenv`

## Setup and Installation

### Prerequisites

* Python 3.8+
* MongoDB Instance (local or Atlas)
* NewsAPI Key
* Google Gemini API Key (for `gemini-2.0-flash-001` model)

### Environment Variables

Create a `.env.backend` file in the root directory of the project (or ensure these are set in your deployment environment):

```
MONGO_URI="your_mongodb_connection_string"
NEWS_API_KEY_SCRAPER="your_newsapi_key"
GOOGLE_API_KEY="your_google_gemini_api_key"
FLASK_ENV="development" # or "production"
FLASK_DEBUG="True" # or "False"
SECRET_KEY="a_strong_secret_key"
SCRAPE_INTERVAL_HOURS=6 # Interval for scheduled scraping/analysis
BATCH_SIZE_ANALYSIS=10 # Number of articles to analyze per batch
```

*Note*: The `.gitignore` file includes `/` .env.backend` to prevent it from being committed.

### Installation

1.  **Clone the repository**:
    ```bash
    git clone https://github.com/Jefino9488/TruthGuard-Backend.git
    cd TruthGuard-Backend
    ```

2.  **Create a virtual environment**:
    ```bash
    python -m venv .venv
    source .venv/bin/activate # On Windows use `.venv\Scripts\activate`
    ```

3.  **Install dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

### Running the Application

```bash
python run.py
```

This will start the Flask development server on `http://0.0.0.0:5000/`. The scheduled scraping and analysis tasks will also begin running in the background.

## Deployment (Google Cloud Run)

The project includes a `cloudbuild.yaml` file for continuous deployment to Google Cloud Run.

To deploy using Google Cloud Build:

1.  **Build the container image**:
    ```yaml
    - name: 'gcr.io/cloud-builders/docker'
      args: ['build', '-t', 'gcr.io/$PROJECT_ID/truthguard-backend', '.']
    ```

2.  **Push the image to Google Container Registry**:
    ```yaml
    - name: 'gcr.io/cloud-builders/docker'
      args: ['push', 'gcr.io/$PROJECT_ID/truthguard-backend']
    ```

3.  **Deploy to Cloud Run**:
    ```yaml
    - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
      entrypoint: gcloud
      args:
        - 'run'
        - 'deploy'
        - 'truthguard-backend'
        - '--image'
        - 'gcr.io/$PROJECT_ID/truthguard-backend'
        - '--region'
        - 'asia-south1'
        - '--platform'
        - 'managed'
        - '--allow-unauthenticated'
        - '--service-account'
        - '377368788327-compute@developer.gserviceaccount.com' # Replace with your service account
        - '--set-secrets'
        - 'MONGODB_URI=MONGODB_URI:latest,NEWS_API_KEY_SCRAPER=NEWS_API_KEY_SCRAPER:latest,GOOGLE_API_KEY=GOOGLE_API_KEY:latest'
        - '--set-env-vars'
        - 'FLASK_ENV=production'
    ```
    This configuration specifies the image, region, platform, allows unauthenticated access, sets a service account, and injects secrets as environment variables.

Additionally, there's a GitHub Actions workflow (`.github/workflows/dispatch-main-repo.yml`) that triggers an `update-submodule` event on the `TruthGuard_Frontend` repository upon a push to the `master` branch of this backend repository.

## API Endpoints

The API is structured around `main_bp` blueprint.

* **GET `/health`**: Checks the health of the application and its dependencies (MongoDB, NewsAPI key, Google AI API key).
* **POST `/scrape`**: Triggers the news scraping task asynchronously.
* **POST `/analyze`**: Triggers the AI analysis task asynchronously on pending articles.
* **POST `/analyze-manual`**: Manually analyzes a given headline/content or a URL. Expects JSON body with `{"headline": "...", "content": "..."}` OR `{"url": "..."}`.
* **GET `/articles`**: Retrieves a paginated list of articles. Supports `page`, `limit`, `sort_by`, `sort_order` query parameters.
* **GET `/articles/<article_id>`**: Retrieves details of a single article by its MongoDB ID.
* **GET `/articles/search?q=<query>`**: Searches articles using MongoDB's text index. Supports `q`, `page`, `limit`, `sort_by`, `sort_order` query parameters.
* **GET `/articles/high-bias`**: Retrieves articles flagged with high bias. Supports `min_score`, `page`, `limit`, `sort_order` query parameters.
* **GET `/articles/misinformation-risk`**: Retrieves articles flagged with high misinformation risk. Supports `min_risk`, `page`, `limit`, `sort_order` query parameters.
* **GET `/dashboard-analytics`**: Retrieves aggregated analytics data for the dashboard, including total articles, average scores, and distribution by bias/source.
* **POST `/vector-search`**: Performs a MongoDB Atlas Vector Search based on a query string. Expects JSON body `{"query": "search term", "limit": 10}`.

## Database (MongoDB Atlas Vector Search)

For the vector search functionality to work, you must manually create a Vector Search Index in your MongoDB Atlas cluster UI or via Atlas CLI/API. The index should be configured on the `content_embedding` and `title_embedding` fields (and optionally `analysis_embedding`) with `knnVector` type, specifying the correct dimensions (e.g., 384 for `all-MiniLM-L6-v2`) and a similarity metric (e.g., `cosine`).

Example index definition:

```json
{
  "mappings": {
    "dynamic": true,
    "fields": {
      "content_embedding": {
        "type": "knnVector",
        "dimensions": 384,
        "similarity": "cosine"
      },
      "title_embedding": {
        "type": "knnVector",
        "dimensions": 384,
        "similarity": "cosine"
      }
    }
  }
}
```
